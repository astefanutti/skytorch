"""
SkyTorch PyTorch backend storage manager - Tracks remote storage allocations.

This module manages storage allocations for sky tensors, tracking storage IDs
and their metadata. Storage IDs are used as proxy data pointers in the
allocator, avoiding actual memory allocation on the client side.
"""

from __future__ import annotations

import asyncio
import logging
import weakref
from collections import defaultdict
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Optional
from weakref import WeakValueDictionary

import torch

from skytorch.torch.backend._async import get_event_loop
from skytorch.torch.client.tensor import get_storage_id, get_tensor_id

if TYPE_CHECKING:
    from skytorch.client.compute import Compute

logger = logging.getLogger(__name__)


@dataclass
class StorageInfo:
    """Information about a storage allocation."""

    nbytes: int
    device_index: int
    _compute_ref: weakref.ref[Compute] = field(repr=False)

    @property
    def compute(self) -> Optional[Compute]:
        """Get the associated Compute, or None if garbage collected."""
        return self._compute_ref()


def _delete_tensors_after_gc(
    compute: "Compute",
    tensor_ids: list[int],
) -> None:
    """
    Delete tensors after garbage collection.

    Called on the event loop thread via call_soon_threadsafe, so no lock
    is held when this executes. This avoids the reentrant deadlock that
    occurs when GC triggers free_storage while _pending_lock is held.
    """
    from skytorch.torch.backend._client import delete_tensors

    async def _do_delete():
        try:
            await delete_tensors(compute, tensor_ids)
        except Exception as e:
            logger.warning(f"Failed to delete tensor(s) {tensor_ids} after GC: {e}")

    asyncio.ensure_future(_do_delete())


class StorageManager:
    """
    Manages storage allocations for sky tensors.

    Storage IDs are simple integers that serve as references to remote storage.
    The data pointer in PyTorch tensors is set to the storage ID cast to void*,
    allowing efficient tracking without actual memory allocation.

    IMPORTANT: Storage registration is lazy - storage IDs are generated by the
    C++ allocator using an atomic counter, but registration with StorageManager
    (including Compute association) is deferred until first tensor use. This
    keeps the allocator GIL-free for PyTorch 2.10 compatibility.
    """

    def __init__(self):
        self._storages: dict[int, StorageInfo] = {}
        self._tensor_to_storage: WeakValueDictionary[int, torch.UntypedStorage] = (
            WeakValueDictionary()
        )
        self._storage_to_tensors: dict[int, set[int]] = defaultdict(set)

    def register_storage(
        self,
        storage_id: int,
        nbytes: int,
        device_index: int,
    ) -> None:
        """
        Register storage lazily if not already registered.

        This is called at first tensor use to register storage that was
        allocated by the C++ allocator. The allocator generates storage IDs
        without calling Python (GIL-free), so registration is deferred here.

        Args:
            storage_id: Storage ID from the C++ allocator
            nbytes: Size of the storage in bytes
            device_index: Device index for the storage
        """
        if storage_id in self._storages:
            return  # Already registered

        from skytorch.torch.backend._device import device_manager

        compute = device_manager.get_compute(device_index)
        if compute is None:
            raise RuntimeError(
                f"No Compute registered for device index {device_index}. "
                "Ensure you have called compute.device() to register the device."
            )

        info = StorageInfo(nbytes, device_index, _compute_ref=weakref.ref(compute))
        self._storages[storage_id] = info

    def free_storage(self, storage_id: int) -> None:
        """
        Free a storage allocation.

        Handles both registered and unregistered storage IDs.
        If the storage was never used (lazy allocation), this is a no-op.
        Schedules tensor deletion on the Compute's event loop (non-blocking).

        Args:
            storage_id: ID of the storage to free
        """
        tensor_ids = list(self._storage_to_tensors.pop(storage_id, set()))
        info = self._storages.pop(storage_id, None)
        if info is None or not tensor_ids:
            return

        # Get the Compute (may be None if garbage collected)
        compute = info.compute
        if compute is None:
            logger.warning(
                f"Compute was garbage collected, skipping deletion of tensor(s) " f"{tensor_ids}"
            )
            return

        # Always defer via call_soon_threadsafe to avoid reentrant deadlock.
        # GC can trigger free_storage while _pending_lock is held (e.g., during
        # PendingRequest allocation in _submit_request_sync). Calling
        # delete_tensors_sync directly here would try to re-acquire the same
        # non-reentrant lock on the same thread, causing a deadlock.
        # By deferring to call_soon_threadsafe, the deletion runs on the event
        # loop thread at a point where no lock is held.
        loop = get_event_loop()
        try:
            loop.call_soon_threadsafe(_delete_tensors_after_gc, compute, tensor_ids)
        except RuntimeError as e:
            logger.warning(f"Failed to schedule deletion for tensor(s) {tensor_ids}: {e}")

    def resize_storage(self, storage_id: int, new_nbytes: int) -> None:
        """
        Resize a storage allocation.

        Args:
            storage_id: ID of the storage to resize
            new_nbytes: New size in bytes
        """
        if storage_id in self._storages:
            self._storages[storage_id].nbytes = new_nbytes

    def get_storage(self, storage_id: int) -> Optional[StorageInfo]:
        """
        Get storage info by ID.

        Args:
            storage_id: ID of the storage

        Returns:
            StorageInfo or None if not found
        """
        return self._storages.get(storage_id)

    def register_tensor(self, tensor: torch.Tensor) -> int:
        """
        Register a tensor with its associated storage.

        Args:
            tensor: The sky tensor to register

        Returns:
            The tensor ID
        """
        tensor_id = get_tensor_id(tensor)
        storage_id = get_storage_id(tensor)
        self._tensor_to_storage[tensor_id] = tensor.untyped_storage()
        self._storage_to_tensors[storage_id].add(tensor_id)
        # Sync to C++ registration set for fast path lookup
        try:
            from skytorch.torch.backend import _C

            _C._register_tensor_id(tensor_id)
            # Also register storage_id â†’ tensor_id mapping for view detection
            _C._register_storage_tensor_mapping(storage_id, tensor_id)
        except (ImportError, AttributeError):
            pass
        return tensor_id

    def tensor_ref(self, tensor: torch.Tensor) -> Optional[int]:
        """
        Get a registered tensor ID for this tensor or its storage.

        Args:
            tensor: The tensor to look up

        Returns:
            The tensor's ID if registered, otherwise the first registered
            tensor ID sharing the same storage, otherwise None
        """
        tensor_id = get_tensor_id(tensor)
        if tensor_id in self._tensor_to_storage:
            return tensor_id

        storage_id = get_storage_id(tensor)
        tensor_ids = self._storage_to_tensors.get(storage_id)
        if tensor_ids:
            return next(iter(tensor_ids))

        return None


# Global storage manager singleton
storage_manager = StorageManager()
