FROM python:3.14-slim

ARG TARGETARCH

USER 1000

# Set working directory
WORKDIR /app

RUN python3 -m venv .venv

ENV VIRTUAL_ENV=/app/.venv
ENV PATH=$VIRTUAL_ENV/bin:$PATH

# Install external dependencies
RUN pip install --no-cache-dir "torch==2.9.1+cu128" --index-url https://download.pytorch.org/whl/cu128
RUN pip install --no-cache-dir "kernels" "transformers" "accelerate" "nvidia-ml-py"

# Install Flash Attention from pre-built wheels (no PyPI wheels for Python 3.14).
# Source: https://github.com/mjun0812/flash-attention-prebuild-wheels
# Pinned: flash-attn 2.8.3, CUDA 12.8, PyTorch 2.9, Python 3.14
RUN WHEEL_ARCH=$([ "$TARGETARCH" = "amd64" ] && echo "x86_64" || echo "aarch64") && \
    pip install --no-cache-dir --only-binary :all: \
    "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu128torch2.9-cp314-cp314-linux_${WHEEL_ARCH}.whl"

# Copy the skytorch Python package
COPY --chown=1000:root pyproject.toml README.md VERSION.md ./
COPY --chown=1000:root skytorch/ skytorch/

# Install project
RUN pip install --no-cache-dir "."

# Set Python path to include the app directory
ENV PYTHONPATH=/app

# Disable stdout/stderr buffering so print() output appears immediately in container logs
ENV PYTHONUNBUFFERED=1

# Server configuration via environment variables
ENV SKYTORCH_PORT=50051
ENV SKYTORCH_HOST=[::]
ENV SKYTORCH_CHUNK_SIZE=1048576
ENV SKYTORCH_LOG_LEVEL=INFO
ENV SKYTORCH_METRICS_SOURCES=nvidia-gpu

# Run the server
ENTRYPOINT ["python", "-m", "skytorch.torch.server"]
