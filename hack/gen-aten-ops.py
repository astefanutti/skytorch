#!/usr/bin/env python3

"""Generate ATen operator registrations for SkyTorch backend."""

import argparse
from pathlib import Path

# Operators already registered in C++ (RegisterAten.cpp)
# These MUST NOT be registered in Python to avoid infinite recursion
CPP_REGISTERED_OPS = {
    "empty.memory_format",
    "empty_strided",
    "view",
    "as_strided",
    "_unsafe_view",
    "_reshape_alias",
    "set_.source_Tensor",
    "set_.source_Storage",
    "set_.source_Storage_storage_offset",
    "resize_",
    "alias",
    "_lazy_clone",
    # View/shape ops â€” purely metadata, no gRPC needed
    "t",
    "transpose.int",
    "permute",
    "expand",
    "squeeze.dim",
    "squeeze.dims",
    "unsqueeze",
    "select.int",
    "slice.Tensor",
}


def generate_registration_code(operators: list[str]) -> str:
    """Generate Python code for operator registrations."""
    lines = [
        '"""Generated ATen operator registrations for SkyTorch backend.',
        "",
        "This file is auto-generated by hack/gen-aten-ops.py.",
        "Do not edit manually.",
        '"""',
        "",
        "from typing import Any, Callable",
        "",
        "import torch",
        "",
        "from .dispatch import _sky_kernel_fallback",
        "",
        "",
        "def _sky_kernel_fallback_wrapper(",
        "    op: torch._ops.OpOverload | torch._ops.OpOverloadPacket,",
        ") -> Callable[..., Any]:",
        '    """Create a wrapper that calls _sky_kernel_fallback with the specified op."""',
        "",
        "    def wrapper(*args: Any, **kwargs: Any) -> Any:",
        "        return _sky_kernel_fallback(op, *args, **kwargs)",
        "",
        "    return wrapper",
        "",
        "",
        "# Register operators with sky backend",
        '_sky_lib_aten = torch.library.Library("aten", "IMPL")',
        "",
    ]

    for op_name in sorted(operators):
        # Handle overload names
        if "." in op_name:
            base, overload = op_name.split(".", 1)
            op_attr = f"torch.ops.aten.{base}.{overload}"
        else:
            op_attr = f"torch.ops.aten.{op_name}.default"

        lines.append("_sky_lib_aten.impl(")
        lines.append(f'    "{op_name}",')
        lines.append(f"    _sky_kernel_fallback_wrapper({op_attr}),")
        lines.append('    dispatch_key="PrivateUse1",')
        lines.append(")")
        lines.append("")

    return "\n".join(lines)


def main():
    parser = argparse.ArgumentParser(description="Generate ATen operator registrations")
    parser.add_argument(
        "-o",
        "--output",
        default="skytorch/torch/backend/aten/ops.py",
        help="Output file path",
    )
    args = parser.parse_args()

    # TODO: Parses PyTorch's native_functions.yaml

    operators = [
        # Convolution (critical for CNN)
        "convolution",
        "convolution.out",
        "convolution_backward",
        "convolution_backward.out",
        # Batch normalization
        "_native_batch_norm_legit",
        "_native_batch_norm_legit.no_stats",
        "_native_batch_norm_legit.no_stats_out",
        "_native_batch_norm_legit.out",
        "_native_batch_norm_legit_no_training",
        "_native_batch_norm_legit_no_training.out",
        "native_batch_norm_backward",
        # Pooling
        "max_pool2d_with_indices",
        "max_pool2d_with_indices.out",
        "max_pool2d_with_indices_backward",
        "max_pool2d_with_indices_backward.grad_input",
        "avg_pool2d",
        "avg_pool2d.out",
        "avg_pool2d_backward",
        "avg_pool2d_backward.grad_input",
        "_adaptive_avg_pool2d",
        "_adaptive_avg_pool2d.out",
        "_adaptive_avg_pool2d_backward",
        "_adaptive_avg_pool2d_backward.out",
        # Activations
        "relu",
        "relu.out",
        "relu_",
        "gelu",
        "gelu.out",
        "gelu_",
        "gelu_backward",
        "gelu_backward.grad_input",
        "_softmax",
        "_softmax.out",
        "_softmax_backward_data",
        "_softmax_backward_data.out",
        "_log_softmax",
        "_log_softmax.out",
        "_log_softmax_backward_data",
        "_log_softmax_backward_data.out",
        "sigmoid",
        "sigmoid.out",
        "sigmoid_",
        "sigmoid_backward",
        "sigmoid_backward.grad_input",
        "tanh",
        "tanh.out",
        "tanh_",
        "tanh_backward",
        "tanh_backward.grad_input",
        "threshold_backward",
        "threshold_backward.grad_input",
        # Basic math
        "add.Tensor",
        "add.Scalar",
        "add.out",
        "add_.Tensor",
        "add_.Scalar",
        "sub.Tensor",
        "sub.Scalar",
        "sub.out",
        "sub_.Tensor",
        "sub_.Scalar",
        "mul.Tensor",
        "mul.Scalar",
        "mul.out",
        "mul_.Tensor",
        "mul_.Scalar",
        "div.Tensor",
        "div.Scalar",
        "div.out",
        "div_.Tensor",
        "div_.Scalar",
        "div.Tensor_mode",
        "div.Scalar_mode",
        # Matrix operations
        "mm",
        "mm.out",
        "bmm",
        "bmm.out",
        "addmm",
        "addmm.out",
        "addmm_",
        # Reductions
        "sum",
        "sum.dim_IntList",
        "sum.out",
        "mean",
        "mean.dim",
        "mean.out",
        "argmax",
        "argmax.out",
        "max",
        "max.dim",
        "max.dim_max",
        # Comparisons
        "eq.Tensor",
        "eq.Scalar",
        "eq.Tensor_out",
        "eq.Scalar_out",
        "ne.Tensor",
        "ne.Scalar",
        "gt.Tensor",
        "gt.Scalar",
        "lt.Tensor",
        "lt.Scalar",
        "ge.Tensor",
        "ge.Scalar",
        "le.Tensor",
        "le.Scalar",
        # Shape operations
        "cat",
        "cat.out",
        # Indexing
        "index.Tensor",
        "index.Tensor_out",
        # TODO
        # "masked_select",
        # "masked_select.out",
        # Dropout
        "native_dropout",
        "native_dropout.out",
        "native_dropout_backward",
        "native_dropout_backward.out",
        # Initialization
        "zeros",
        "zeros.out",
        "ones",
        "ones.out",
        "full",
        "full.out",
        "fill_.Scalar",
        "fill_.Tensor",
        "zero_",
        # Other common ops
        "clone",
        # NOTE: copy_ is NOT registered here - it's handled specially by _copy_from
        # in copy.py which handles cross-device copies (sky<->cpu, sky<->sky)
        "neg",
        "neg.out",
        "neg_",
        "abs",
        "abs.out",
        "abs_",
        "sqrt",
        "sqrt.out",
        "sqrt_",
        "rsqrt",
        "rsqrt.out",
        "rsqrt_",
        "exp",
        "exp.out",
        "exp_",
        "log",
        "log.out",
        "log_",
        "pow.Tensor_Scalar",
        "pow.Tensor_Scalar_out",
        "pow.Tensor_Tensor",
        "pow.Tensor_Tensor_out",
        "pow.Scalar",
        "pow.Scalar_out",
        "pow_.Scalar",
        # Logical (needed by pow backward decomposition)
        "logical_and",
        "logical_and.out",
        "logical_and_",
        # Conditional (needed by pow backward decomposition)
        "where.self",
        "where.self_out",
        # Scalar tensor creation (needed by backward decompositions)
        "scalar_tensor",
        "scalar_tensor.out",
        # Cross entropy loss components
        "nll_loss_forward",
        "nll_loss_forward.output",
        "nll_loss_backward",
        "nll_loss_backward.grad_input",
        "nll_loss2d_forward",
        "nll_loss2d_forward.output",
        "nll_loss2d_backward",
        "nll_loss2d_backward.grad_input",
        # Embedding
        "embedding",
        "embedding_dense_backward",
        # Linear algebra
        # FIXME
        # "linear",
        # "linear_backward",
    ]

    # Filter out any C++ registered operators (safety check)
    filtered_operators = [op for op in operators if op not in CPP_REGISTERED_OPS]
    if len(filtered_operators) < len(operators):
        excluded = set(operators) - set(filtered_operators)
        print(f"Excluded {len(excluded)} C++ registered operators: {excluded}")

    code = generate_registration_code(filtered_operators)

    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(code)
    print(f"Generated {len(filtered_operators)} operator registrations to {output_path}")


if __name__ == "__main__":
    main()
